---
title: 'SO-ARM101 Pick & Place 튜토리얼'
description: 'SO-ARM101과 ACT로 물체 집어 담기 작업 학습하기 튜토리얼'
---

import Callout from '../../../../../../components/Callout.astro';
import Tabs from '../../../../../../components/Tabs.astro';

# SO-ARM101 Pick & Place 튜토리얼

이 튜토리얼에서는 SO-ARM101 로봇과 ACT(Action Chunking with Transformers) 모델을 사용하여 물체를 집어서 바구니에 담는 작업을 학습시키는 전체 과정을 다룹니다.

<Callout type="info" title="필요 사항">
  - **하드웨어**: SO-ARM101 (리더+팔로워)
  - **카메라**: USB 웹캠 2대 (top-view, wrist-view)
  - **작업 물체**: 작은 블록 또는 펜
  - **목표 바구니**: 플라스틱 통 또는 박스 또는 연필 통 등
</Callout>

## 작업 목표

로봇이 테이블 위의 물체를 집어서 바구니에 담는 작업을 자율적으로 수행하도록 학습시킵니다.

## 1. 환경 설정

### 작업 공간 준비

<Callout type="tip" title="설정 팁">
  - **카메라 위치**: 작업 공간 전체가 보이도록 Top-view 각도로 설치
  - **바구니 선택**: 플라스틱 통, 박스 또는 연필 통 등 사용
  - **조명 설정**: 그림자가 적게 생기도록 균일한 조명
  - **작업 공간**: 로봇 팔 기준 반경 30cm 이내
</Callout>

### SO-ARM101 연결 및 테스트
```bash
# 1. 디바이스 확인
lerobot-find-port

# 2. 리더-팔로워 연결 및 카메라 테스트
lerobot-teleoperate \
  --teleop.type=so101_leader \
  --teleop.port=/dev/so101_leader \
  --teleop.id=leader \
  --robot.type=so101_follower \
  --robot.port=/dev/so101_follower \
  --robot.id=follower \
  --robot.cameras='{ 
      top: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 25},
      wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 25},
  }' \
  --display_data=true
```

## 2. 데이터 수집

### 텔레옵 작업 녹화
```bash
# Pick & Place 데이터셋 생성
export TASK_NAME="pick_and_place_basket"
export HF_USER="your_username"

lerobot-record \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM0 \
    --teleop.id=leader \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM1 \
    --robot.id=follower \
    --robot.cameras='{ 
        top: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 25},
        wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 25},
    }' \
    --dataset.single_task=${TASK_NAME} \
    --dataset.repo_id=${HF_USER}/${TASK_NAME} \
    --dataset.num_episodes=45 \
    --dataset.episode_time_s=15 \
    --dataset.reset_time_s=3 \
    --display_data=true
```

### 데이터 수집 프로토콜

<Callout type="warning" title="중요한 시연 규칙">
  1. **시작 위치**: 로봇 팔은 항상 홈 포지션에서 시작
  2. **물체 위치**: 5개의 다른 위치에서 각 20회씩 시연
  3. **속도**: 일정한 속도로 부드럽게 움직이기
  4. **그립**: 
      - 물체를 확실하게 잡았는지 확인 후 이동
      - 물체를 잡고 이동하는 일련의 과정 일관성
</Callout>

---

## 3. 데이터 검증

```bash
# 수집한 데이터 시각화
lerobot-dataset-viz \
    --repo-id=${HF_USER}/${TASK_NAME} \
    --episode-index=0 \
    --mode=distant \
    --web-port=8080


# 브라우저에서 http://localhost:8080 접속
```

## 4. ACT 모델 학습

### 학습 설정
```bash
# ACT 모델 학습 시작
python -m lerobot.scripts.train \
    --dataset.repo_id=${HF_USER}/${TASK_NAME} \
    --policy.type=act \
    --output_dir=outputs/act_${TASK_NAME} \
    --policy.device=cuda \
    --training.num_epochs=500 \
    --training.batch_size=32 \
    --training.learning_rate=1e-4 \
    --wandb.enable=true \
    --wandb.project="so101_pick_place"
```

### 학습 모니터링
```python
# 학습 중 W&B에서 실시간으로 확인할 수 있는 메트릭:
# - loss/total: 전체 손실
# - loss/action: 액션 예측 손실
# - validation/success_rate: 검증 성공률
```

## 5. 모델 평가 및 실행

### 실시간 평가
```bash
# 학습된 모델로 실제 작업 수행
python -m lerobot.scripts.eval \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyUSB0 \
    --robot.cameras="webcam" \
    --policy.path=outputs/act_${TASK_NAME}/checkpoint_best.pt \
    --dataset.num_episodes=20 \
    --dataset.repo_id=${HF_USER}/${TASK_NAME}_eval
```

### 성공률 측정
```bash
# 20회 테스트 후 성공률 자동 계산
# 성공 기준: 물체가 바구니 안에 들어감
```

## 6. 문제 해결 및 최적화

### 일반적인 문제와 해결책

<Callout type="warning" title="낮은 성공률 (70% 미만)">
  **원인**: 데이터 부족 또는 일관성 부족
  
  **해결책**:
  - 실패한 위치에서 추가 20개 데이터 수집
  - 속도를 더 일정하게 유지
  - 그립 강도 조정
</Callout>

<Callout type="danger" title="물체를 놓치는 경우">
  **원인**: 그립 타이밍 문제
  
  **해결책**:
  ```bash
  # 그립 지속 시간 증가
  python -m lerobot.scripts.train \
      --policy.action_chunk_size=50 \
      --resume
  ```
</Callout>

## 결과 공유

```bash
# HuggingFace에 모델 업로드
huggingface-cli upload ${HF_USER}/act_so101_pick_place \
    outputs/act_${TASK_NAME}/checkpoint_best.pt

# 데모 비디오 생성
python -m lerobot.scripts.create_demo_video \
    --policy.path=outputs/act_${TASK_NAME}/checkpoint_best.pt \
    --output=demo_pick_place.mp4
```

## 기대 성능

| 지표 | 목표값 | 일반적 달성값 |
|-----|-------|------------|
| 성공률 | 90% 이상 | 85-95% |
| 평균 수행 시간 | 10초 이내 | 8-12초 |
| 필요 데이터 | 100 에피소드 | 80-120 |

<Callout type="success" title="축하합니다!">
  SO-ARM101으로 첫 번째 Pick & Place 작업을 성공적으로 학습시켰습니다!
  
  **다음 도전 과제**:
  - 다양한 물체 종류로 확장
  - 여러 개의 바구니 구분하기
  - 색상별 분류 작업
</Callout>

---

*이 튜토리얼의 전체 코드와 데이터셋은 [HuggingFace Hub](https://huggingface.co/lerobot)에서 확인할 수 있습니다.*